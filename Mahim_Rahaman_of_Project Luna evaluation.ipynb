{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFZmUtk4FCnP"
   },
   "source": [
    "## 5. Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdMogOg0FF7F"
   },
   "source": [
    "5.1 Obtain the model's prediction on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxFOCkdvFFtm",
    "outputId": "c8ecfceb-65d1-4b85-acc7-f4ad20de83c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Test Loss: 1.6853, Test Accuracy: 99.90%\n"
     ]
    }
   ],
   "source": [
    " model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "\n",
    "            out = model(img)\n",
    "\n",
    "            loss = loss_fn(out, label)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRRTayzmFLTy"
   },
   "source": [
    "5.2 Calculate the report the following metrics:\n",
    "- accuracy\n",
    "- precision\n",
    "- recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEJDyFKpFScI"
   },
   "source": [
    "5.3: Discuss the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWjsJxE9FYlb"
   },
   "outputs": [],
   "source": [
    "#This model has performed exceptionally well. I have had to tinker with the design a lot with diffrent loss functions,learning rate. I realized I did not do the data loader properly which is why my code did not run well.\n",
    "#finally I was able to achive a 99% accurary which is increadible in this context. I was expecting a 70% accuracy but somehow adding x = self.relu2(x) x = self.fc3(x), these 2 parameters did the job.\n",
    "#The lost primarily was not stable with numbers over 1000 which was extreme. Later I had to look into the tranning and testing losses symontaniusly which allowed me better understadning and I adjusted accordningly.I am happy with the results now I want to check it out visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZfRj3rpqk1D"
   },
   "source": [
    "## 6. Data Augmentation and Retraining\n",
    "\n",
    "To enhance the model's performance, it is essential to increase the number of malignant instances. Apply random shifting and rotation to generate new training instances, ensuring an equal number of instances in each class within the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZTGy-K5rf85"
   },
   "source": [
    "6.1 Augment the number of malignent instances in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RK3-cio9rlo8",
    "outputId": "e8870fec-9d8f-4c02-d414-882669da77ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented batch shape: torch.Size([32, 32, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "])\n",
    "\n",
    "  # positive class\n",
    "    train_chunks_augmented = []\n",
    "    train_labels_augmented = []\n",
    "\n",
    "    for img, label in zip(train_chunks, train_labels):\n",
    "      if label == 1:\n",
    "        augmented_img = augmentation_transforms(img)\n",
    "        train_chunks_augmented.append(augmented_img)\n",
    "        train_labels_augmented.append(label)\n",
    "    for img, label in zip(train_chunks, train_labels):\n",
    "      if label == 0:\n",
    "        train_chunks_augmented.append(img)\n",
    "        train_labels_augmented.append(label)\n",
    "\n",
    "  # augmented data into tensors\n",
    "    train_chunks_augmented = torch.stack(train_chunks_augmented)\n",
    "    train_labels_augmented = torch.tensor(train_labels_augmented, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(train_chunks_augmented, train_labels_augmented)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    print(f\"Augmented batch shape: {next(iter(train_loader))[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxsuW3iLrmFC"
   },
   "source": [
    "6.2 Retrain the neural network model on the new training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhwG0PcersGy",
    "outputId": "35a8b934-1cd5-443e-8bd7-d359dadf6c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 83.8977\n",
      "Epoch 1/20, Test Loss: 55.8334, Test Accuracy: 99.80%\n",
      "Epoch 2/20, Training Loss: 47.5911\n",
      "Epoch 2/20, Test Loss: 6.0229, Test Accuracy: 99.90%\n",
      "Epoch 3/20, Training Loss: 10.4297\n",
      "Epoch 3/20, Test Loss: 9.2878, Test Accuracy: 98.50%\n",
      "Epoch 4/20, Training Loss: 7.9931\n",
      "Epoch 4/20, Test Loss: 12.8115, Test Accuracy: 99.90%\n",
      "Epoch 5/20, Training Loss: 10.9177\n",
      "Epoch 5/20, Test Loss: 16.2670, Test Accuracy: 99.90%\n",
      "Epoch 6/20, Training Loss: 4.8276\n",
      "Epoch 6/20, Test Loss: 13.0005, Test Accuracy: 99.90%\n",
      "Epoch 7/20, Training Loss: 0.2658\n",
      "Epoch 7/20, Test Loss: 8.0462, Test Accuracy: 99.90%\n",
      "Epoch 8/20, Training Loss: 0.0884\n",
      "Epoch 8/20, Test Loss: 8.8153, Test Accuracy: 99.90%\n",
      "Epoch 9/20, Training Loss: 0.0008\n",
      "Epoch 9/20, Test Loss: 8.5869, Test Accuracy: 99.90%\n",
      "Epoch 10/20, Training Loss: 0.0008\n",
      "Epoch 10/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 11/20, Training Loss: 0.0008\n",
      "Epoch 11/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 12/20, Training Loss: 0.0008\n",
      "Epoch 12/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 13/20, Training Loss: 0.0008\n",
      "Epoch 13/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 14/20, Training Loss: 0.0008\n",
      "Epoch 14/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 15/20, Training Loss: 0.0008\n",
      "Epoch 15/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 16/20, Training Loss: 0.0007\n",
      "Epoch 16/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 17/20, Training Loss: 0.0007\n",
      "Epoch 17/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 18/20, Training Loss: 0.0007\n",
      "Epoch 18/20, Test Loss: 8.5868, Test Accuracy: 99.90%\n",
      "Epoch 19/20, Training Loss: 0.0007\n",
      "Epoch 19/20, Test Loss: 8.5955, Test Accuracy: 99.90%\n",
      "Epoch 20/20, Training Loss: 0.0007\n",
      "Epoch 20/20, Test Loss: 8.6047, Test Accuracy: 99.90%\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size_2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_size = 73728\n",
    "hidden_size_1 = 128\n",
    "hidden_size_2 = 64\n",
    "output_size = 2\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size_1, hidden_size_2, output_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for img, label in train_loader:\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "        out = model(img)\n",
    "\n",
    "\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "            out = model(img)\n",
    "\n",
    "\n",
    "            loss = loss_fn(out, label)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofURek20rsco"
   },
   "source": [
    "6.3 Perform model evaluation and compare the performance of the new model to the old model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7ZEVZP8r0X1"
   },
   "outputs": [],
   "source": [
    "\"\"\"I’m honestly pretty surprised by the results. When I compared the two models, the old one performed way better than I expected. The training loss started at 83.8977 and gradually dropped all the way down to 0.0007,\n",
    "with the test loss also going down and stabilizing at a very low value. The test accuracy remained a solid 99.90% throughout, which really shows how well it learned and generalized. But the new model, it's a bit of a shocker.\n",
    "The training loss dropped to 0.0007 quickly, but the test loss just got stuck at 1.6853, even though the test accuracy stayed at 99.90%. It feels like the new model is just underfitting, not improving much despite its accuracy.\n",
    "I’m really surprised that the old model outperforms it so much in terms of optimization and learning!\n",
    "\"\"\"\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
